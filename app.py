import os
import torch
import gradio as gr
import faiss
import pickle
import numpy as np
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from sentence_transformers import SentenceTransformer, CrossEncoder
from threading import Thread
from rank_bm25 import BM25Okapi
import wikipedia

# ==============================================================================
# 1. C·∫§U H√åNH
# ==============================================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))


MODEL_ID = "unsloth/Qwen2.5-3B-Instruct-bnb-4bit"

# ƒê∆∞·ªùng d·∫´n Database
RAG_INDEX_PATH = os.path.join(BASE_DIR, "rag_builder", "results", "history_vector.index")
RAG_META_PATH = os.path.join(BASE_DIR, "rag_builder", "results", "history_metadata.pkl")

# C·∫•u h√¨nh Logic
RERANK_TOP_K = 5  # Ch·ªâ l·∫•y 5 k·∫øt qu·∫£ t·ªët nh·∫•t ƒë·ªÉ rerank
RAG_THRESHOLD = 0.9

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üñ•Ô∏è Main DeGvice: {device.upper()}")

# ==============================================================================
# 2. LOAD COMPONENT
# ==============================================================================
try:
    print("‚è≥ Loading Models (Embedder & Reranker) on CPU to save VRAM...")
    embedder = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder', device=device)
    reranker = CrossEncoder('BAAI/bge-reranker-v2-m3', max_length=512, device=device)
    print("‚úÖ Embedder & Reranker Ready (CPU Mode)!")
except Exception as e:
    print(f"‚ùå L·ªói load Embedder/Reranker: {e}")
    embedder, reranker = None, None

rag_index = None
rag_metadata = []
bm25 = None

if os.path.exists(RAG_INDEX_PATH):
    print("‚è≥ Loading Database...")
    rag_index = faiss.read_index(RAG_INDEX_PATH)
    with open(RAG_META_PATH, "rb") as f:
        rag_metadata = pickle.load(f)

    print("‚è≥ Initializing BM25 Search...")
    try:
        tokenized_corpus = [doc['original_query'].lower().split() for doc in rag_metadata]
        bm25 = BM25Okapi(tokenized_corpus)
        print("‚úÖ Hybrid Search Ready (BM25 Loaded)!")
    except Exception as e:
        print(f"‚ùå L·ªói BM25: {e}")

# ==============================================================================
# LOAD QWEN-3B-4BIT
# ==============================================================================
print(f"‚è≥ Loading Qwen-3B-4bit from {MODEL_ID}...")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print(f"‚úÖ Model 3B Ready! (VRAM optimized)")

    # D·ªçn d·∫πp RAM sau khi load
    torch.cuda.empty_cache()
    gc.collect()

except Exception as e:
    print(f"‚ùå Model Error: {e}")
    model = None


# ==============================================================================
# 3. H√ÄM PH·ª§ TR·ª¢: T√åM KI·∫æM WEB & HYBRID
# ==============================================================================
# ==============================================================================
# 3. CH·ª®C NƒÇNG TH√îNG MINH: SUY LU·∫¨N T·ª™ KH√ìA & TRA WIKI
# ==============================================================================
def extract_keywords(user_query):
    """
    d√πng Python c·∫Øt b·ªè c√°c t·ª´ ƒë·ªÉ h·ªèi th·ª´a th√£i.
    """
    print(f"‚ö° ƒêang l·ªçc t·ª´ kh√≥a nhanh cho: {user_query}")

    # Danh s√°ch c√°c t·ª´ r√°c th∆∞·ªùng g·∫∑p trong c√¢u h·ªèi
    stop_phrases = [
        "cho t√¥i h·ªèi", "cho m√¨nh h·ªèi", "b·∫°n c√≥ bi·∫øt", "h√£y cho bi·∫øt",
        "l√† g√¨", "l√† ai", "nh∆∞ th·∫ø n√†o", "t·∫°i sao", "khi n√†o", "·ªü ƒë√¢u", "bao nhi√™u",
        "√Ω nghƒ©a c·ªßa", "nguy√™n nh√¢n", "di·ªÖn bi·∫øn", "k·∫øt qu·∫£", "t√≥m t·∫Øt",
        "c√≥", "nh·ªØng", "c√°c", "c√°i", "g√¨", "?", "!",
        # TH√äM C√ÅC T·ª™ N·ªêI N√ÄY:
        "trong", "cu·ªôc", "c·ªßa", "v·ªÅ", "vi·ªác", "ƒë√£", "ƒëang", "s·∫Ω", "·ªü", "t·∫°i", "b·ªã", "ƒë∆∞·ª£c"
    ]

    # 1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng ƒë·ªÉ x·ª≠ l√Ω
    clean_text = user_query.lower()

    # 2. X√≥a c√°c t·ª´ r√°c
    for phrase in stop_phrases:
        clean_text = clean_text.replace(phrase, "")

    # 3. Chu·∫©n h√≥a l·∫°i (x√≥a kho·∫£ng tr·∫Øng th·ª´a)
    clean_text = " ".join(clean_text.split())

    # 4. N·∫øu x√≥a h·∫øt tr∆°n (c√¢u h·ªèi qu√° ng·∫Øn), th√¨ l·∫•y l·∫°i c√¢u g·ªëc
    if len(clean_text) < 2:
        clean_text = user_query

    print(f"üëâ T·ª´ kh√≥a nhanh: '{clean_text}'")
    return clean_text


def search_wikipedia(query):
    """T√¨m Top 1 b√†i v√† ƒê·ªçc 3000 k√Ω t·ª± ƒë·∫ßu ti√™n t·ª´ Wikipedia Ti·∫øng Vi·ªát."""
    print(f"üåê Tra c·ª©u Wiki (Deep Read): {query}")
    wikipedia.set_lang("vi")

    try:
        # 1. T√¨m ti√™u ƒë·ªÅ b√†i vi·∫øt kh·ªõp nh·∫•t
        search_results = wikipedia.search(query, results=1)

        if not search_results:
            print("   --> Wiki kh√¥ng t√¨m th·∫•y b√†i n√†o.")
            return ""

        title = search_results[0]
        print(f"   --> ƒêang ƒë·ªçc b√†i: {title}")

        # 2. Truy c·∫≠p v√†o trang ƒë·ªÉ l·∫•y n·ªôi dung ƒë·∫ßy ƒë·ªß
        page = wikipedia.page(title, auto_suggest=False)

        # 3. L·∫•y 3000 k√Ω t·ª± ƒë·∫ßu ti√™n (Ch·ª©a Intro + Infobox + Ch∆∞∆°ng 1)
        content = page.content[:3000]

        # X·ª≠ l√Ω xu·ªëng d√≤ng cho g·ªçn
        clean_content = content.replace("\n\n", "\n")

        return f"NGU·ªíN: Wikipedia Ti·∫øng Vi·ªát ({title})\nN·ªòI DUNG TR√çCH D·∫™N:\n{clean_content}..."

    except wikipedia.DisambiguationError as e:
        # N·∫øu t·ª´ kh√≥a chung chung, l·∫•y b√†i ƒë·∫ßu ti√™n trong g·ª£i √Ω
        try:
            first_opt = e.options[0]
            page = wikipedia.page(first_opt, auto_suggest=False)
            content = page.content[:2000]
            return f"NGU·ªíN: Wikipedia ({first_opt})\nN·ªòI DUNG:\n{content}..."
        except:
            return ""

    except Exception as e:
        print(f"‚ùå L·ªói Wiki: {e}")
        return ""
def hybrid_search(query, top_k=15):
    vec_candidates = []
    if rag_index and embedder:
        vec = embedder.encode([query])
        D, I = rag_index.search(np.array(vec).astype('float32'), k=top_k)
        for idx in I[0]:
            if idx != -1:
                vec_candidates.append(idx)

    bm25_candidates = []
    if bm25:
        try:
            tokenized_query = query.lower().split()
            doc_scores = bm25.get_scores(tokenized_query)
            top_n = np.argsort(doc_scores)[::-1][:top_k]
            bm25_candidates = top_n.tolist()
        except:
            pass

    all_indexes = list(set(vec_candidates + bm25_candidates))

    final_candidates = []
    seen_answers = set()

    for idx in all_indexes:
        if idx < len(rag_metadata):
            item = rag_metadata[idx]
            ans = item.get('answer', '').strip()
            if ans not in seen_answers and len(ans) > 10:
                seen_answers.add(ans)
                final_candidates.append([query, ans])

    return final_candidates


# ==============================================================================
# 4. BOT RESPONSE (LOGIC TH√îNG MINH)
# ==============================================================================
def bot_response(message, history):
    if model is None:
        history = history or []
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "‚ùå L·ªói: Model ch∆∞a ƒë∆∞·ª£c load"})
        return history, ""

    final_context = ""
    source_label = ""

    # D·ªçn d·∫πp VRAM tr∆∞·ªõc khi sinh
    torch.cuda.empty_cache()

    try:
        # B∆Ø·ªöC 1: T√¨m trong RAG n·ªôi b·ªô
        candidates = hybrid_search(message, top_k=15)

        if candidates and reranker:
            scores = reranker.predict(candidates)
            scored_candidates = []
            for i, score in enumerate(scores):
                scored_candidates.append({"score": score, "text": candidates[i][1]})

            scored_candidates.sort(key=lambda x: x['score'], reverse=True)

            # L·∫•y Top 5
            top_candidates = scored_candidates[:RERANK_TOP_K]

            if top_candidates:
                best = top_candidates[0]
                # B∆Ø·ªöC 2: Ki·ªÉm tra ng∆∞·ª°ng ƒëi·ªÉm
                if best['score'] > RAG_THRESHOLD:
                    final_context = best['text']
                    source_label = "üìö D·ªØ li·ªáu n·ªôi b·ªô"
                    print(f"‚úÖ CH·ªêT RAG (Score {best['score']:.2f} > {RAG_THRESHOLD})")
                else:
                    print(f"‚ö†Ô∏è ƒêi·ªÉm RAG th·∫•p ({best['score']:.2f} < {RAG_THRESHOLD}) -> Chuy·ªÉn sang Web Search...")


                    wiki_keyword = extract_keywords(message)


                    wiki_content = search_wikipedia(wiki_keyword)

                    if wiki_content:
                        final_context = wiki_content
                        source_label = f"üåê Wikipedia (T·ª´ kh√≥a: {wiki_keyword})"
                        print("‚úÖ WIKI SUCCESS")
                    else:
                        print("‚ùå Wiki failed.")
    except Exception as e:
        print(f"Quy tr√¨nh Search g·∫∑p l·ªói: {e}")

    # T·∫°o Prompt
    if final_context:
        prompt = f"""### T√ÄI LI·ªÜU THAM KH·∫¢O:
    {final_context}

    ### CH·ªà TH·ªä TUY·ªÜT ƒê·ªêI:
    B·∫°n l√† tr·ª£ l√Ω l·ªãch s·ª≠ ng∆∞·ªùi Vi·ªát. H√£y tr·∫£ l·ªùi c√¢u h·ªèi theo quy t·∫Øc:
    1. **Ngu·ªìn tin:** ∆Øu ti√™n d√πng [T√ÄI LI·ªÜU] (Ngu·ªìn: {source_label}).
    2. **B·ªï sung:** N·∫øu t√†i li·ªáu thi·∫øu, h√£y d√πng ki·∫øn th·ª©c c·ªßa b·∫°n v√† n√≥i "Theo ki·∫øn th·ª©c c·ªßa t√¥i...".
    3. **Ng√¥n ng·ªØ:** CH·ªà D√ôNG TI·∫æNG VI·ªÜT. C·∫•m tuy·ªát ƒë·ªëi ti·∫øng Trung/Anh.
    4. **VƒÉn phong:** Ng·∫Øn g·ªçn, s√∫c t√≠ch, ƒëi th·∫≥ng v√†o c√¢u tr·∫£ l·ªùi.

    ### C√ÇU H·ªéI:
    {message}

    ### TR·∫¢ L·ªúI:"""
    else:
        # Fallback (Khi kh√¥ng t√¨m th·∫•y g√¨ c·∫£)
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω l·ªãch s·ª≠ ng∆∞·ªùi Vi·ªát.
    Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi ng·∫Øn g·ªçn, ch√≠nh x√°c b·∫±ng Ti·∫øng Vi·ªát.
    L∆∞u √Ω: N·∫øu kh√¥ng bi·∫øt, h√£y n√≥i "T√¥i kh√¥ng bi·∫øt". KH√îNG ƒê∆Ø·ª¢C B·ªäA ƒê·∫∂T hay d√πng ti·∫øng n∆∞·ªõc ngo√†i.

    C√¢u h·ªèi: {message}
    Tr·∫£ l·ªùi:"""

    # Qwen Chat Template
    messages = [
        {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω AI h·ªØu √≠ch."},
        {"role": "user", "content": prompt}
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)

    attention_mask = (input_ids != tokenizer.pad_token_id).long()

    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    generation_kwargs = dict(
        input_ids=input_ids,
        attention_mask=attention_mask,
        streamer=streamer,
        max_new_tokens=256,
        temperature=0.1,
        top_p=0.95,
        repetition_penalty=1.1,
        do_sample=False,  # Greedy Search ƒë·ªÉ trung th·ª±c
        pad_token_id=tokenizer.eos_token_id
    )

    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    if history is None:
        history = []

    history.append({"role": "user", "content": message})

    partial_text = f"({source_label})\n" if final_context else ""
    for new_text in streamer:
        partial_text += new_text
        temp_history = history[:-1] + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": partial_text}
        ]
        yield temp_history, ""

    history.append({"role": "assistant", "content": partial_text})
    yield history, ""


# ==============================================================================
# 5. CUSTOM CSS (GI·ªÆ NGUY√äN UI C·ª¶A B·∫†N)
# ==============================================================================
custom_css = """
<style>
body {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

.gradio-container {
    max-width: 1200px !important;
    margin: 2rem auto !important;
}

.contain {
    background: rgba(255, 255, 255, 0.95) !important;
    border-radius: 20px !important;
    box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3) !important;
    padding: 2rem !important;
}

h1 {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    font-size: 2.5rem !important;
    font-weight: 800 !important;
    text-align: center !important;
    margin-bottom: 0.5rem !important;
}

.chatbot {
    border-radius: 15px !important;
    border: none !important;
    box-shadow: 0 4px 16px rgba(0, 0, 0, 0.1) !important;
}

button {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
    color: white !important;
    border: none !important;
    border-radius: 12px !important;
    padding: 12px 24px !important;
    font-weight: 600 !important;
    transition: all 0.3s ease !important;
    box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4) !important;
}

button:hover {
    transform: translateY(-2px) !important;
    box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6) !important;
}

textarea {
    border-radius: 12px !important;
    border: 2px solid #e2e8f0 !important;
    padding: 12px !important;
    font-size: 1rem !important;
}

textarea:focus {
    border-color: #667eea !important;
    box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1) !important;
    outline: none !important;
}

.footer {
    text-align: center;
    color: #666;
    margin-top: 2rem;
    font-size: 0.9rem;
}

.example-btn {
    background: white !important;
    color: #667eea !important;
    border: 2px solid #667eea !important;
    margin: 5px !important;
}

.example-btn:hover {
    background: #667eea !important;
    color: white !important;
}
</style>
"""

# ==============================================================================
# 6. GRADIO INTERFACE
# ==============================================================================
with gr.Blocks() as demo:
    gr.HTML(custom_css)

    gr.Markdown(
        """
        # üáªüá≥ S·ª≠ Vi·ªát AI - Tr·ª£ L√Ω L·ªãch S·ª≠ Th√¥ng Minh
        ### H·ªá th·ªëng Hybrid RAG + Qwen 3B (Web Fallback)
        *T√¨m ki·∫øm k·∫øt h·ª£p Vector Search & BM25 | Reranking v·ªõi Cross-Encoder*
        """
    )

    chatbot = gr.Chatbot(label="üí¨ Tr√≤ chuy·ªán", height=500)

    with gr.Row():
        msg = gr.Textbox(
            label="",
            placeholder="üí≠ H·ªèi t√¥i v·ªÅ l·ªãch s·ª≠ Vi·ªát Nam...",
            show_label=False,
            lines=2
        )

    with gr.Row():
        submit = gr.Button("üì§ G·ª≠i", variant="primary")
        clear = gr.Button("üóëÔ∏è X√≥a")

    gr.Markdown("### üí° C√¢u h·ªèi g·ª£i √Ω:")
    with gr.Row():
        example1 = gr.Button("B√°c H·ªì ra ƒëi t√¨m ƒë∆∞·ªùng c·ª©u n∆∞·ªõc nƒÉm n√†o?", elem_classes="example-btn")
        example2 = gr.Button("√ù nghƒ©a chi·∫øn th·∫Øng ƒêi·ªán Bi√™n Ph·ªß tr√™n kh√¥ng?", elem_classes="example-btn")

    gr.Markdown(
        """
        <div class='footer'>
        üí° <b>M·∫πo s·ª≠ d·ª•ng:</b> ƒê·∫∑t c√¢u h·ªèi c·ª• th·ªÉ v·ªÅ l·ªãch s·ª≠ Vi·ªát Nam ƒë·ªÉ nh·∫≠n c√¢u tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t<br>
        ‚ö° Powered by Qwen 2.5-3B + Vietnamese Embedder + Reranker
        </div>
        """
    )

    # Event handlers
    msg.submit(bot_response, [msg, chatbot], [chatbot, msg])
    submit.click(bot_response, [msg, chatbot], [chatbot, msg])
    clear.click(lambda: ([], ""), None, [chatbot, msg])

    example1.click(lambda: "B√°c H·ªì ra ƒëi t√¨m ƒë∆∞·ªùng c·ª©u n∆∞·ªõc nƒÉm n√†o?", None, msg)
    example2.click(lambda: "√ù nghƒ©a chi·∫øn th·∫Øng ƒêi·ªán Bi√™n Ph·ªß tr√™n kh√¥ng?", None, msg)

if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False
    )