import json
import faiss
import numpy as np
import pickle
import os
import torch
from sentence_transformers import SentenceTransformer

# ---------------------------------------------------------
# C·∫§U H√åNH
# ---------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
# S·ª≠a t√™n file n√†y cho ƒë√∫ng v·ªõi file b·∫°n ƒëang c√≥ (json hay jsonl)
DATA_PATH = os.path.join(current_dir, '..', 'data', 'rawhistory_200k.jsonl')

OUTPUT_INDEX_PATH = os.path.join(current_dir, 'results', 'history_vector.index')
OUTPUT_META_PATH = os.path.join(current_dir, 'results', 'history_metadata.pkl')


# ---------------------------------------------------------
# 1. H√ÄM ƒê·ªåC FILE (CH·∫§P NH·∫¨N M·ªåI ƒê·ªäNH D·∫†NG)
# ---------------------------------------------------------
def load_and_process(file_path):
    print(f"üìÇ ƒêang ƒë·ªçc file: {file_path}")

    documents = []
    metadata = []

    total_lines = 0
    valid_pairs = 0

    # ƒê·ªçc theo ki·ªÉu JSON Lines (M·ªói d√≤ng 1 object) - Ph·ªï bi·∫øn v·ªõi file l·ªõn
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: continue

            total_lines += 1
            try:
                # N·∫øu file l√† m·∫£ng JSON l·ªõn [] th√¨ d√≤ng n√†y s·∫Ω l·ªói, ta x·ª≠ l√Ω ·ªü except
                item = json.loads(line)

                # --- LOGIC TR√çCH XU·∫§T (N·ªöI L·ªéNG) ---
                msgs = item.get("messages", [])
                user_q = ""
                assist_a = ""

                for m in msgs:
                    if m['role'] == 'user':
                        user_q = m['content']
                    elif m['role'] == 'assistant':
                        # L·∫§Y LU√îN! Kh√¥ng c·∫ßn check 'final' n·ªØa
                        # N·∫øu c√≥ nhi·ªÅu c√¢u tr·∫£ l·ªùi, l·∫•y c√¢u cu·ªëi c√πng (th∆∞·ªùng l√† c√¢u ch·ªët)
                        assist_a = m['content']

                if user_q and assist_a:
                    documents.append(user_q)
                    metadata.append({
                        "original_query": user_q,
                        "answer": assist_a
                    })
                    valid_pairs += 1

            except json.JSONDecodeError:
                # Tr∆∞·ªùng h·ª£p file l√† m·ªôt c·ª•c JSON Array l·ªõn [...]
                pass

    # N·∫øu ƒë·ªçc t·ª´ng d√≤ng th·∫•t b·∫°i (valid_pairs = 0), th·ª≠ ƒë·ªçc ki·ªÉu JSON Array to√†n c·ª•c
    if valid_pairs == 0:
        print("‚ö†Ô∏è ƒê·ªçc theo d√≤ng kh√¥ng ƒë∆∞·ª£c, chuy·ªÉn sang ƒë·ªçc to√†n b·ªô file (JSON Array)...")
        f = open(file_path, 'r', encoding='utf-8')
        try:
            items = json.load(f)
            total_lines = len(items)
            for item in items:
                msgs = item.get("messages", [])
                user_q = ""
                assist_a = ""
                for m in msgs:
                    if m['role'] == 'user':
                        user_q = m['content']
                    elif m['role'] == 'assistant':
                        assist_a = m['content']

                if user_q and assist_a:
                    documents.append(user_q)
                    metadata.append({"original_query": user_q, "answer": assist_a})
                    valid_pairs += 1
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc file: {e}")
        finally:
            f.close()

    print(f"{'=' * 30}")
    print(f"üìä T·ªîNG K·∫æT D·ªÆ LI·ªÜU:")
    print(f"   - T·ªïng s·ªë d√≤ng/item ƒë√£ qu√©t: {total_lines}")
    print(f"   - S·ªë c·∫∑p Q-A h·ª£p l·ªá l·∫•y ƒë∆∞·ª£c: {valid_pairs}")
    print(f"{'=' * 30}")

    return documents, metadata


# ---------------------------------------------------------
# 2. CH·∫†Y BUILD
# ---------------------------------------------------------
def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üñ•Ô∏è Device: {device}")

    # A. X·ª≠ l√Ω d·ªØ li·ªáu
    docs, metas = load_and_process(DATA_PATH)

    if not docs:
        print("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu n√†o!")
        return

    # B. Load Model
    print("üöÄ Loading Embedder...")
    embedder = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder', device=device)

    # C. Encode
    print(f"‚ö° ƒêang m√£ h√≥a {len(docs)} c√¢u (Batch 128)...")
    embeddings = embedder.encode(docs, batch_size=128, show_progress_bar=True, convert_to_numpy=True)

    # D. Save FAISS
    print("üì¶ Creating Index...")
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)

    faiss.write_index(index, OUTPUT_INDEX_PATH)
    with open(OUTPUT_META_PATH, "wb") as f:
        pickle.dump(metas, f)

    print("üéâ XONG!")


if __name__ == "__main__":
    main()